# Data Sources Overview

This reference explains every CSV emitted by the GDPR enforcement data pipeline and how each file is produced. Follow the phase order to trace provenance.

## Phase 1 – Extraction (`outputs/phase1_extraction/`)
- `main_dataset.csv`: Generated by `python3 scripts/1_parse_ai_responses.py`. Contains 1,473 rows × 77 columns mapped directly from `raw_data/AI_analysis/AI-responses.txt` (one row per decision with all schema fields). Use as the canonical starting point for validation and repair.
- `data_with_errors.csv`: Also produced by the parser when answers are incomplete. Holds 45 malformed bundles (mostly truncated after Q73) kept for manual follow-up, never fed into later phases.

## Phase 2 – Validation (`outputs/phase2_validation/`)
- `validated_data.csv`: Output of `python3 scripts/2_validate_dataset.py` without arguments (742 rows in the latest run). Lists the subset of Phase 1 rows that pass every schema rule and cross-field constraint.
- `validation_errors.csv`: Detailed error ledger (row id, field, value, rule, severity) emitted by the validator for the rows that failed (2,800 issues logged across 731 rows in the latest run). Drives rule design in Phase 3.
- `validation_report.txt`: Human-readable summary of the validation run (row counts, error totals, top offending fields).
- `enum_analysis.csv` / `enum_analysis.txt`: Produced by `python3 scripts/2_analyze_enum_values.py` to show frequency of valid vs invalid enum tokens across all columns.

## Phase 3 – Repair (`outputs/phase3_repair/`)
- `repaired_dataset.csv`: Created by `python3 scripts/3_repair_data_errors.py`. Applies six targeted enum repairs to the Phase 1 dataset using the Phase 2 error log.
- `repair_log.txt`: Companion log that lists every applied repair (row id, field, from → to, pattern name).
- `repaired_dataset_validated.csv`: Generated by re-running `python3 scripts/2_validate_dataset.py --input outputs/phase3_repair/repaired_dataset.csv`. Contains the 941 post-repair rows that pass all validation checks.
- `repaired_dataset_validation_errors.csv`: Validation failures that remain after repairs (2,026 total issues across 532 rows); use this to decide whether to extend pattern coverage or perform manual curation.
- `repaired_dataset_validation_report.txt`: Summary stats for the post-repair validation pass (mirrors the Phase 2 report format).

## Phase 4 – Enrichment (`outputs/phase4_enrichment/`)
- `0_fx_conversion_metadata.csv`: Diagnostics table showing the FX lookup path (monthly, annual, fallback) for every fine and turnover entry along with source year/month and converted amounts.
- `0_fx_missing_review.csv`: Manual-review helper listing rows where a nominal amount existed but no FX rate was available.
- `1_enriched_master.csv`: Feature-complete master table (one row per decision) with temporal granularity, inferred dates, FX-normalized fines/turnover, 2025 EUR deflators, sanction profiles, Art. 5/83 indicators, context flags, OSS geography, QA signals, and keyword metadata.
- `2_processing_contexts.csv`: Long table of processing contexts with decision IDs and positional order.
- `3_vulnerable_groups.csv`: Long table of vulnerable group mentions per decision.
- `4_guidelines.csv`: Long table capturing guidelines cited in each decision.
- `5_articles_breached.csv`: Parsed GDPR article references including numeric article identifiers, preserved detail tokens, and sequence order.
- `graph/nodes_*.csv`, `graph/edges_*.csv`: Neo4j bulk-import ready node and edge files linking decisions to authorities, defendants, articles, guidelines, and processing contexts.

## Phase 5 – Analysis (`outputs/phase5_analysis/`)
- `0_case_level_features.csv`: Baseline analytical table containing parsed article sets, article-family keys, measure sets, and log fines (2025 EUR).
- `1_baseline_article_cohorts.csv`: Exact article-set cohorts with summary statistics (log fine mean/median, measure-set Jaccard, sanction mode).
- `2_case_level_with_components.csv`: Case-level data annotated with relaxed cohort identifiers (Jaccard ≥ 0.8) for sparsity-aware pooling.
- `2_relaxed_article_components.csv`: Aggregated statistics for each relaxed component (members, counts, average log fines, sanction mode).
- `3_context_effects.csv`: Matched within-cohort contrasts for context flags using Mann–Whitney tests and measure-set Jaccard comparisons.
- `3_legal_basis_effects.csv`: Art. 6 invalid vs valid/NOT_DISCUSSED comparisons with stratum-level medians and sanction modes.
- `3_defendant_type_effects.csv`: PRIVATE vs PUBLIC comparisons restricted to fixed context bundles (employment, CCTV, marketing, etc.).
- `4_cross_country_pairs.csv`: Nearest-neighbour matches across countries within shared article cohorts including log-fine and measure Jaccard for each pair.
- `4_cross_country_summary.csv`: Paired statistics aggregated by article cohort (paired t, McNemar counts, average measure similarity).
- `5_mixed_effects_results.csv`: Coefficient tables from mixed-effects regressions on log fines (full, no-context, no-legal-basis variants).
- `5_mixed_effects_summary.txt`: Statsmodels textual summaries for the three model variants (warnings retained for transparency).
- `6_relaxed_cohort_contrasts.csv`: Sensitivity of context contrasts when swapping exact for relaxed cohorts.
- `6_time_controls_summary.csv`: Pre-2021 vs 2021+ averages of log fines, measure counts, and fine incidence by article cohort.

### Reference Tables (`raw_data/reference/`)
- `fx_rates.csv`: ECB-inspired annual FX rates (and identity rows for EUR) used to standardize fines/turnover to euro values.
- `hicp_ea19.csv`: Euro area HICP index (2016–2025) supporting deflation to constant 2025 euros.
- `context_taxonomy.csv`: Ordered mapping of processing-context tokens to boolean flag columns.
- `region_map.csv`: Analyst-defined mapping from country codes to regional groupings.

Keep this document close when sharing datasets or designing new automation so collaborators can confirm which artefacts are authoritative for a given workflow stage.
